package com.bimarian.main;

import java.util.HashMap;
import java.util.Map;

import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka.KafkaUtils;

import scala.Tuple2;

import com.bimarian.beans.TruckDeviceBean;
import com.bimarian.sources.PropertyFileReader;
import com.cloudera.spark.hbase.JavaHBaseContext;
import com.google.gson.Gson;
import com.google.gson.GsonBuilder;

/**
 * TenantHandler program contains main() consumes data from kafka topics
 * 
 * To run TenantHandler class we need to pass the following parameters as command line arguments:
 * zkQuorum, kafkaGroupName, number of threads for each topic, properties file, duration 
 *  
 * Read tenant's information by using propertyFileReader class
 * 
 * Read all the kafka topics, number of threads to process the topics by using Map
 * 
 * Create an input stream that pulls messages form Kafka topics
 * import KafkaUtils and create an input DStream
 * 
 * Filter JavaDStream data based on tenant name
 * Convert DStream data into RDD's
 */

public class TenantHandler {

	static Logger log = Logger.getLogger(TenantHandler.class.getName());
	static Gson gson = new GsonBuilder().setDateFormat("yyyy-MM-dd'T'HH:mm:ss").create();
	
	static JavaSparkContext javaSparkContext;
	static String topicNames;
	static String defaultTenant;
	static String[] tenantNames;
	
	public static void main(String args[]) {
		if(args.length<5){
			log.error("Usage: com.bimarian.main.TenantHandler <zkQuorum> <kafkaGroupName> <number of threads for each topic> <properties file> <duration>");
			System.exit(1);
		}

		//Initializing a SparkContext
		SparkConf sparkConf = new SparkConf().setAppName("Truck").setMaster("local[2]");
		
		//Create a JavaStreamingContext using a SparkConf configuration.
		//sparkConf -  A Spark application configuration
		//new Duration(Integer.parseInt(args[4])) - The time interval at which streaming data will be divided into batches	
		JavaStreamingContext javaStreamingContext = new JavaStreamingContext(sparkConf, new Duration(Integer.parseInt(args[4])));
		
		//Returns The underlying SparkContext
		javaSparkContext = javaStreamingContext.sparkContext();
		
		PropertyFileReader propertyFileReader =  PropertyFileReader.getInstance(args[3]);
		topicNames = propertyFileReader.getProperty("tenants");
		defaultTenant = propertyFileReader.getProperty("default_tenant");
		tenantNames = propertyFileReader.getProperty("tenants").split(",");
		
		//Creates a Configuration with HBase resources
		//Returns: a Configuration with HBase resources
		org.apache.hadoop.conf.Configuration configuration = HBaseConfiguration.create();
		final JavaHBaseContext javaHBaseContext = new JavaHBaseContext(javaSparkContext, configuration);
		
		int numThreads = Integer.parseInt(args[2]);
		Map<String,Integer> topicMap = new HashMap<String,Integer>();
		String[] topics = topicNames.split(",");
		for(String topic: topics) {
			topicMap.put(topic, numThreads);
		}
		
		/*
		 * JavaPairReceiverInputDStream - A Java-friendly interface to [[org.apache.spark.streaming.dstream.ReceiverInputDStream]], 
		 * the abstract class for defining any input stream that receives data over the network.
		 */
		
		//Create an input stream that pulls messages from a Kafka Broker.
		//Args - JavaStreamingContext, ZookeeperQuorum, KafkaGroupName, topicMap - Map containing numThreads which corresponds to #partitions for each topic, where topic is tenant name.
		JavaPairReceiverInputDStream<String, String> events = KafkaUtils.createStream(javaStreamingContext, args[0], args[1], topicMap);
		
		/* JavaDStream - A Java-friendly interface to DStream, the basic abstraction in Spark Streaming that represents a continuous stream of data. 
		* DStreams can either be created from live data (such as, data from TCP sockets, Kafka, Flume, etc.) or it can be generated by transforming existing DStreams using operations such as map, window.
		*/  
		// Filter event data stream by tenant's name "VRL"
		// map - Return a new DStream by applying a function to all elements of this DStream.
		// filter - Return a new DStream containing only the elements that satisfy a predicate.
		JavaDStream<TruckDeviceBean> truckDataLines = events.map(new Function<Tuple2<String, String>, TruckDeviceBean>() {
			private static final long serialVersionUID = 2975847029458879885L;
			FilterEvents truckDataFilter = new FilterEvents();
			public TruckDeviceBean call(Tuple2<String, String> tuple2) {
				return truckDataFilter.truckEventsFilter(tuple2);	
			}
		}).filter(new Function<TruckDeviceBean, Boolean>() {
			private static final long serialVersionUID = -3072557652358864987L;
			public Boolean call(TruckDeviceBean bean) throws Exception {
				if(bean != null) {
					return true;
				}
					return false;
			}
		});
		
		//foreach - Apply a function to each RDD in this DStream. 
		//This is an output operator, so 'this' DStream will be registered as an output stream and therefore materialized.
		// Processing VRL tenant data 
		final TruckManager truckEvents = new TruckManager();	
		truckDataLines.foreach(new Function<JavaRDD<TruckDeviceBean>, Void>() {
			private static final long serialVersionUID = 8542164426732985469L;
			public Void call(JavaRDD<TruckDeviceBean> truckRdd) throws Exception {
				truckEvents.pushRawDataToHBase(javaHBaseContext, truckRdd);
				return null;
			}
		});
		
		/* print() - Print the first ten elements of each RDD generated in this DStream. 
		 * This is an output operator, so this DStream will be registered as an output stream and there materialized
		 */
		truckDataLines.print();
		//Start the execution of the streams.
		javaStreamingContext.start();
		//Wait for the execution to stop. Any exceptions that occurs during the execution will be thrown in this thread.
		javaStreamingContext.awaitTermination();
	}
}
